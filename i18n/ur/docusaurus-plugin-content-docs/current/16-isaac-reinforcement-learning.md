---
sidebar_position: 16
---

# Isaac کے ساتھ روبوٹ کنٹرول کے لیے ری انفورسمنٹ لرننگ

**ری انفورسمنٹ لرننگ (Reinforcement Learning - RL)** مشین لرننگ میں ایک طاقتور پیراڈائم ہے جہاں ایک "ایجنٹ" ماحول میں اعمال انجام دے کر جمع شدہ "انعام" کو زیادہ سے زیادہ کرنے کے لیے فیصلے کرنا سیکھتا ہے۔ روبوٹکس کے لیے، RL خاص طور پر دلچسپ ہے کیونکہ یہ روبوٹ کو آزمائش اور غلطی کے ذریعے پیچیدہ رویوں کو سیکھنے کی اجازت دیتا ہے، بغیر کسی واضح پروگرامنگ کے۔

NVIDIA Isaac Sim روبوٹک ری انفورسمنٹ لرننگ کے لیے ایک بہترین پلیٹ فارم ہے کیونکہ یہ ہزاروں روبوٹ ایجنٹوں کو متوازی طور پر نقل کر سکتا ہے، جس سے سیکھنے کے عمل میں ڈرامائی طور پر تیزی آتی ہے۔

## ری انفورسمنٹ لرننگ کے بنیادی تصورات

1.  **ایجنٹ (Agent)**: سیکھنے والا یا فیصلہ ساز۔ ہمارے معاملے میں، روبوٹ۔
2.  **ماحول (Environment)**: وہ دنیا جس میں ایجنٹ کام کرتا ہے۔ یہ Isaac Sim میں سمیلیٹڈ دنیا ہے۔
3.  **حالت (State - S)**: ایک خاص لمحے میں ماحول کا ایک اسنیپ شاٹ۔ ایک روبوٹ کے لیے، اس میں اس کے جوائنٹ اینگلز، ویلوسیٹی، اور سینسر ریڈنگز شامل ہو سکتی ہیں۔
4.  **ایکشن (Action - A)**: ایجنٹ کے ذریعے کیا گیا ایک فیصلہ۔ ایک روبوٹ کے لیے، یہ اس کے جوائنٹس پر لاگو کرنے کے لیے ٹارکس یا ویلوسیٹیز ہوں گی۔
5.  **انعام (Reward - R)**: ایک اسکیلر فیڈ بیک سگنل جو ماحول ایجنٹ کو فراہم کرتا ہے۔ انعام ایجنٹ کو بتاتا ہے کہ کسی دی گئی حالت میں اس کا آخری عمل کتنا اچھا یا برا تھا۔ ایجنٹ کا مقصد وقت کے ساتھ حاصل ہونے والے کل انعام کو زیادہ سے زیادہ کرنا ہے۔
6.  **پالیسی (Policy - π)**: ایجنٹ کا "دماغ"۔ یہ ایک فنکشن ہے جو ایک حالت کو ایک عمل سے نقشہ کرتا ہے (یعنی، یہ ایجنٹ کو بتاتا ہے کہ کسی بھی صورتحال میں کیا کرنا ہے)۔ RL کا مقصد بہترین پالیسی تلاش کرنا ہے۔

## Isaac Sim میں روبوٹک RL ورک فلو

Isaac Sim، `orbit` جیسے ٹولز کے ساتھ مل کر، روبوٹس کے لیے RL پالیسیوں کو تربیت دینے کے لیے ایک ہموار ورک فلو فراہم کرتا ہے۔

### 1. ٹاسک اور ریوارڈ فنکشن کی وضاحت کریں

یہ RL عمل کا سب سے اہم اور تخلیقی حصہ ہے۔ آپ کو یہ وضاحت کرنی ہوگی کہ آپ روبوٹ سے کیا کروانا چاہتے ہیں اور اسے پیشرفت کے لیے کیسے انعام دینا ہے۔

**مثال: ایک روبوٹ بازو کو ہدف تک پہنچنے کے لیے تربیت دینا**

-   **حالت (State)**: حالت میں بازو کے جوائنٹ پوزیشنز اور ویلوسیٹیز، اور ہدف کی 3D پوزیشن شامل ہو سکتی ہے۔
-   **ایکشن (Action)**: ایکشن بازو کے ہر جوائنٹ کے لیے ہدف کی ویلوسیٹیز ہوں گی۔
-   **ریوارڈ فنکشن (Reward Function)**: ایک سادہ ریوارڈ فنکشن روبوٹ کے اینڈ ایفیکٹر (اس کا "ہاتھ") اور ہدف کے درمیان منفی فاصلے پر مبنی ہو سکتا ہے۔
    -   `reward = -distance(end_effector, target)`
    -   اس طرح، جیسے جیسے روبوٹ ہدف کے قریب آتا جائے گا، فاصلہ کم ہوتا جائے گا، اور انعام (ایک کم منفی عدد) بڑھتا جائے گا۔
    -   آپ ہدف تک پہنچنے کے لیے ایک بڑا مثبت انعام اور ہر ٹائم اسٹیپ کے لیے ایک چھوٹا منفی انعام بھی شامل کر سکتے ہیں تاکہ روبوٹ کو کام کو تیزی سے حل کرنے کی ترغیب دی جا سکے۔

### 2. ایک متوازی سمولیشن ماحول بنائیں

حقیقی وقت میں سیکھنے والا ایک واحد روبوٹ ناقابل یقین حد تک سست ہو سکتا ہے۔ روبوٹکس کے لیے عملی RL کی کلید **متوازی سازی (parallelization)** ہے۔ Isaac Sim کے ساتھ، آپ اپنے روبوٹ اور اس کے ماحول کی سیکڑوں یا ہزاروں کاپیاں بنا سکتے ہیں۔ ان میں سے ہر ایک سمیلیٹڈ روبوٹ ایک آزاد "ورکر" کے طور پر کام کرتا ہے، جو متوازی طور پر تجربہ جمع کرتا ہے۔

یہ بڑے پیمانے پر متوازی سازی سیکھنے کے الگورتھم کو کم وقت میں بہت زیادہ ڈیٹا جمع کرنے کی اجازت دیتا ہے، جس سے تربیت کے عمل کو ہفتوں یا مہینوں سے صرف چند گھنٹوں تک ڈرامائی طور پر تیز کیا جا سکتا ہے۔

### 3. پالیسی کو تربیت دیں

متوازی ماحول کو ایک RL ٹریننگ فریم ورک کے ذریعے منظم کیا جاتا ہے۔ Isaac Sim `rl_games` اور `stable-baselines3` جیسی مشہور RL لائبریریوں کے ساتھ مربوط ہے۔ تربیت کا عمل اس طرح کام کرتا ہے:

1.  ہر متوازی ایجنٹ اپنے سمیلیٹڈ ماحول سے اپنی موجودہ حالت کا مشاہدہ کرتا ہے۔
2.  تمام ایجنٹوں سے حالتوں کا بیچ موجودہ پالیسی (جو ایک نیورل نیٹ ورک ہے) کو کھلایا جاتا ہے۔
3.  پالیسی ہر ایجنٹ کے لیے ایک عمل کو آؤٹ پٹ کرتی ہے۔
4.  ہر ایجنٹ اپنے ماحول میں اپنا عمل انجام دیتا ہے۔
5.  ماحول ہر ایجنٹ کے لیے انعام کا حساب لگاتا ہے اور اس کی نئی حالت کا تعین کرتا ہے۔
6.  تمام ایجنٹوں سے یہ `(State, Action, Reward, New State)` ڈیٹا جمع کیا جاتا ہے اور پالیسی کے نیورل نیٹ ورک کو اپ ڈیٹ کرنے کے لیے استعمال کیا جاتا ہے، عام طور پر **پراکسیمل پالیسی آپٹیمائزیشن (PPO)** جیسے الگورتھم کا استعمال کرتے ہوئے۔
7.  لاکھوں مراحل کے لیے دہرائیں۔

### 4. تربیت یافتہ پالیسی کو تعینات کریں

تربیت کے بعد، حتمی پالیسی ایک انتہائی آپٹمائزڈ نیورل نیٹ ورک ہے جو سینسر ڈیٹا (حالت) کو لے کر ریئل ٹائم میں موٹر کمانڈز (عمل) کو آؤٹ پٹ کر سکتی ہے۔ یہ پالیسی پھر ایک فزیکل روبوٹ پر تعینات کی جا سکتی ہے۔ چونکہ اسے مختلف سمیلیٹڈ حالات (ڈومین رینڈمائزیشن کی بدولت) پر تربیت دی گئی تھی، یہ حقیقی دنیا کے لیے زیادہ مضبوط اور قابل موافقت ہوگی۔

ری انفورسمنٹ لرننگ روبوٹکس کے لیے ایک گیم چینجر ہے، اور Isaac Sim جیسے سمیلیٹر ہی اس کی صلاحیت کو کھولنے کی کلید ہیں، جو ہمیں ایسے رویوں کے لیے کنٹرول پالیسیوں کو خود بخود دریافت کرنے کے قابل بناتے ہیں جنہیں ہاتھ سے پروگرام کرنا تقریباً ناممکن ہوگا۔