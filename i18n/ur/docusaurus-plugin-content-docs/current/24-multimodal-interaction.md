---
sidebar_position: 24
---

# ملٹی موڈل تعامل کی طاقت: تقریر، اشارہ، اور بصارت

انسانی مواصلت شاذ و نادر ہی ایک ہی چینل تک محدود ہوتی ہے۔ جب ہم بات کرتے ہیں، تو ہم اپنے ہاتھوں کو اشارہ کرنے کے لیے، اپنی آنکھوں کو رابطہ قائم کرنے کے لیے، اور اپنی پوزیشن کو معنی پہنچانے کے لیے بھی استعمال کرتے ہیں۔ ایک روبوٹ کے لیے حقیقی طور پر قدرتی اور مؤثر معاون بننے کے لیے، اسے **ملٹی موڈل تعامل (multi-modal interaction)** کو سمجھنے اور استعمال کرنے کے قابل ہونا چاہیے — جو مواصلات کے متعدد چینلز، بنیادی طور پر تقریر، اشارے، اور بصارت کا ہموار امتزاج ہے۔

## ملٹی موڈل کیوں؟

مواصلات کے ایک واحد موڈ پر انحصار کرنا اکثر غیر مؤثر یا مبہم ہوتا ہے۔ درج ذیل منظرناموں پر غور کریں:

-   **تقریر کی ابہام (Ambiguity of Speech)**: اگر آپ کہتے ہیں، "وہ کپ مجھے دو،" جبکہ میز پر تین کپ پڑے ہوں، تو صرف تقریر ناکافی ہے۔
-   **اشارے کی ابہام (Ambiguity of Gesture)**: اگر آپ صرف ایک بھرپور میز کی طرف اشارہ کرتے ہیں، تو یہ واضح نہیں ہوتا کہ آپ کس چیز کی طرف اشارہ کر رہے ہیں۔

تاہم، اگر آپ کہتے ہیں، "وہ کپ مجھے دو،" *اور ساتھ ہی* سرخ کپ کی طرف اشارہ کرتے ہیں، تو تقریر اور اشارے کا امتزاج ابہام کو بالکل حل کر دیتا ہے۔ روبوٹ آپ کی نیت کا صحیح اندازہ لگانے کے لیے معلومات کے ان دو دھاروں کو فیوز کر سکتا ہے۔

## موڈز کا فیوژن

ملٹی موڈل تعامل **سینسر فیوژن (sensor fusion)** کا مسئلہ ہے۔ روبوٹ کو مختلف سینسرز (تقریر کے لیے مائیکروفونز، بصارت کے لیے کیمرے) سے ڈیٹا لینا چاہیے اور انہیں صارف کے کمانڈ کی ایک واحد، مربوط سمجھ میں فیوز کرنا چاہیے۔

### کلیدی چینلز اور ان کے کردار

1.  **تقریر (Speech)**: جیسا کہ ہم نے دیکھا ہے، تقریر کمانڈ کے اعلیٰ سطحی سیمنٹک مواد (جو "کیا") فراہم کرتی ہے۔ یہ تجریدی تصورات، اعمال، اور خصوصیات کو پہنچانے کے لیے اچھی ہے۔

2.  **بصارت (Vision)**: بصارت حقیقی دنیا میں زبان کی اہم بنیاد فراہم کرتی ہے۔ روبوٹ کا کیمرہ اسے یہ کرنے کی اجازت دیتا ہے:
    -   **اشیاء کی شناخت (Identify Objects)**: "سرخ کپ" کو سمجھنے کے لیے، روبوٹ کو میز پر موجود اشیاء کو دیکھنے اور ایک کو کپ کے طور پر شناخت کرنے اور اس کے رنگ کو سرخ کے طور پر درجہ بندی کرنے کے قابل ہونا چاہیے۔
    -   **انسانوں کو ٹریک کرنا (Track Humans)**: روبوٹ کو صارف کو یہ سمجھنے کے لیے دیکھنے کی ضرورت ہے کہ کون بات کر رہا ہے اور وہ کہاں ہیں۔
    -   **اشاروں کو پڑھنا (Read Gestures)**: بصارت وہ ہے جس کے ذریعے روبوٹ صارف کے اشاروں کو دیکھتا ہے۔

3.  **اشارہ (Gesture)**: اشارے، خاص طور پر اشارہ کرنا، ماحول میں اشیاء کو ڈی ریفرینس کرنے کا ایک طاقتور ذریعہ ہیں (جو "کہاں")۔
    -   **اشارہ کی شناخت (Gesture Recognition)**: روبوٹ کے پرسیپشن سسٹم کو ایک اشارہ کی شناخت کرنے کے قابل ہونا چاہیے۔ یہ عام طور پر کیمرہ امیج پر **انسانی پوز ایسٹیمیشن (human pose estimation)** ماڈل (جیسے OpenPose) کو چلا کر کیا جاتا ہے۔ یہ ماڈل انسانی جسم کے کلیدی جوڑوں (کندھے، کہنیاں، کلائیاں، وغیرہ) کی شناخت کرتا ہے۔
    -   **اشارے کو چیز سے جوڑنا (Connecting Gesture to Object)**: ایک بار جب ایک اشارہ کا پتہ چل جاتا ہے، تو روبوٹ صارف کے کندھے سے، ان کی انگلی کے ذریعے، اور 3D دنیا میں ایک رے پروجیکٹ کر سکتا ہے۔ پہلی چیز جو یہ رے کاٹتی ہے وہ شاید وہ چیز ہے جس کا حوالہ دیا جا رہا ہے۔

## ایک ملٹی موڈل تعامل کا منظرنامہ

آئیے دیکھتے ہیں کہ ایک روبوٹ کمانڈ، "وہ وہاں رکھو،" پر کیسے عمل کرے گا۔

1.  **صارف کا عمل (User Action)**: صارف کہتا ہے، "وہ رکھو،" ایک کتاب کی طرف اشارہ کرتے ہوئے، اور پھر کہتا ہے، "...وہاں،" ایک بک شیلف پر خالی جگہ کی طرف اشارہ کرتے ہوئے۔

2.  **مسلسل حس (Continuous Sensing)**: روبوٹ مسلسل چل رہا ہے:
    -   **ASR**: آڈیو کو ٹرانسکرائب کرنا۔
    -   **آبجیکٹ ڈیٹیکشن (Object Detection)**: اپنی نظر میں تمام اشیاء کی شناخت کرنا (مثلاً، `book_1`، `bookshelf_1`)۔
    -   **انسانی پوز ایسٹیمیشن (Human Pose Estimation)**: صارف کے جسم کی پوزیشن کو ٹریک کرنا۔

3.  **واقعہ فیوژن (Event Fusion)**: روبوٹ کے تعامل مینیجر کو ان واقعات کو وقت میں ہم آہنگ کرنا چاہیے۔
    -   **واقعہ 1**: روبوٹ لفظ "وہ" سنتا ہے اور، اسی وقت، ایک اشارہ کا پتہ لگاتا ہے۔ یہ اشارہ رے کا حساب لگاتا ہے اور پاتا ہے کہ یہ `book_1` سے کٹتا ہے۔ اب یہ سمجھتا ہے کہ "وہ" = `book_1` ہے۔
    -   **واقعہ 2**: روبوٹ پھر لفظ "وہاں" سنتا ہے اور ایک دوسرا اشارہ کا پتہ لگاتا ہے۔ یہ اس نئی رے کا حساب لگاتا ہے اور پاتا ہے کہ یہ `bookshelf_1` پر ایک خالی جگہ سے کٹتا ہے۔ اب یہ سمجھتا ہے کہ "وہاں" = `location_on_bookshelf_1` ہے۔

4.  **ٹاسک کی تشکیل (Task Formulation)**: تعامل مینیجر کے پاس اب ایک مکمل طور پر مخصوص، غیر مبہم کمانڈ ہے: `putDown(book_1, location_on_bookshelf_1)`۔

5.  **عمل درآمد (Execution)**: یہ کمانڈ روبوٹ کے موشن پلانر اور کنٹرولر کو پاس کی جاتی ہے، جو کام کو عمل میں لاتا ہے۔

## HRI کا مستقبل

HRI کا مستقبل مزید بھرپور ملٹی موڈل تجربات تخلیق کرنے میں ہے۔ اس میں شامل ہو سکتا ہے:
-   **نظر کو سمجھنا (Understanding Gaze)**: روبوٹ صرف صارف کی آنکھوں کی حرکات کی پیروی کرکے یہ اندازہ لگا سکتا ہے کہ وہ کیا چاہتا ہے۔
-   **آواز کے لہجے کی تشریح (Interpreting Tone of Voice)**: روبوٹ یہ پہچان سکتا ہے کہ آیا صارف خوش ہے، مایوس ہے، یا اپنے آواز کے لہجے کی بنیاد پر سوال پوچھ رہا ہے۔
-   **ہیپٹک فیڈ بیک (Haptic Feedback)**: روبوٹ لمس کے ذریعے بات چیت کر سکتا ہے، مثال کے طور پر، صارف کے کندھے پر آہستہ سے تھپتھپا کر ان کی توجہ حاصل کرنے کے لیے۔

ایسے روبوٹس بنا کر جو انسانی مواصلات کے مکمل سپیکٹرم کو سمجھ سکتے ہیں، ہم ایسی مشینیں بنا سکتے ہیں جو نہ صرف زیادہ قابل ہیں بلکہ زیادہ بدیہی، باہمی تعاون پر مبنی، اور ہماری زندگیوں کے تانے بانے میں بغیر کسی رکاوٹ کے ضم ہیں۔