"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[2015],{2231(e,n,o){o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"vla-intro","title":"Voice-to-Action (VLA) for Humanoids","description":"Integrating voice commands into humanoid robots transforms how humans interact with intelligent machines. Voice-to-Action (VLA) systems enable robots to understand spoken language, interpret intent, and execute physical tasks, bridging the gap between human instruction and robotic behavior.","source":"@site/docs/01-vla-intro.md","sourceDirName":".","slug":"/vla-intro","permalink":"/docs/vla-intro","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac","permalink":"/docs/isaac-intro"},"next":{"title":"ROS 2 Nodes and Topics","permalink":"/docs/nodes-topics"}}');var t=o(2540),s=o(3023);const a={sidebar_position:1},r="Voice-to-Action (VLA) for Humanoids",c={},l=[{value:"The VLA Pipeline for Humanoids",id:"the-vla-pipeline-for-humanoids",level:2},{value:"OpenAI Whisper for Speech Recognition",id:"openai-whisper-for-speech-recognition",level:2},{value:"Large Language Models (LLMs) for Planning",id:"large-language-models-llms-for-planning",level:2},{value:"Capstone Humanoid Voice Agent",id:"capstone-humanoid-voice-agent",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-to-action-vla-for-humanoids",children:"Voice-to-Action (VLA) for Humanoids"})}),"\n",(0,t.jsx)(n.p,{children:"Integrating voice commands into humanoid robots transforms how humans interact with intelligent machines. Voice-to-Action (VLA) systems enable robots to understand spoken language, interpret intent, and execute physical tasks, bridging the gap between human instruction and robotic behavior."}),"\n",(0,t.jsx)(n.h2,{id:"the-vla-pipeline-for-humanoids",children:"The VLA Pipeline for Humanoids"}),"\n",(0,t.jsx)(n.p,{children:"A typical VLA pipeline for a humanoid robot involves several stages:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition (ASR)"}),": Converts spoken audio into text. Technologies like OpenAI Whisper are highly effective for this."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Processes the recognized text to extract meaning, intent, and entities. Large Language Models (LLMs) are central here, capable of understanding complex commands and context."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Planning"}),": Translates the understood intent into a sequence of robot actions. This often involves mapping NLU output to specific ROS 2 services or actions. For humanoids, this can include bipedal locomotion, manipulation, or expressive gestures."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robot Execution"}),": The robot's control system executes the planned actions in the physical world."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"openai-whisper-for-speech-recognition",children:"OpenAI Whisper for Speech Recognition"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper is a general-purpose speech recognition model that can transcribe audio into text in various languages and even translate them. Its robust performance makes it an excellent choice for the ASR component of a VLA system."}),"\n",(0,t.jsx)(n.h2,{id:"large-language-models-llms-for-planning",children:"Large Language Models (LLMs) for Planning"}),"\n",(0,t.jsx)(n.p,{children:"LLMs have revolutionized NLU and action planning. By leveraging LLMs, humanoid robots can:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand complex commands"}),": Interpret ambiguous or multi-step instructions that go beyond simple keywords."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Contextual awareness"}),": Use dialogue history to refine understanding and generate more appropriate responses."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code Generation (LLM-to-ROS 2)"}),": Some LLMs can directly translate natural language commands into code snippets (e.g., Python for ",(0,t.jsx)(n.code,{children:"rclpy"}),") or a sequence of ROS 2 commands, simplifying the action planning stage."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-humanoid-voice-agent",children:"Capstone Humanoid Voice Agent"}),"\n",(0,t.jsx)(n.p,{children:"The ultimate goal of this module is to build a capstone project: a humanoid voice agent. This agent will demonstrate the full VLA pipeline, allowing users to issue voice commands that the robot interprets and acts upon, possibly involving navigation, object interaction, or expressive communication. This project ties together concepts from ROS 2, simulation, and advanced AI models."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},3023(e,n,o){o.d(n,{R:()=>a,x:()=>r});var i=o(3696);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);