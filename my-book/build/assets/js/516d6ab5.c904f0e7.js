"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[1047],{6043(e,s,n){n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"sensor-systems","title":"A Guide to Humanoid Sensor Systems","description":"For a humanoid robot to perceive and navigate its environment, it must rely on a sophisticated suite of sensors. These sensors are the robot\'s equivalent of human senses, providing the raw data needed to see, hear, feel, and balance. In ROS 2, each of these sensors is typically managed by a dedicated node that publishes its data to the rest of the system.","source":"@site/docs/06-sensor-systems.md","sourceDirName":".","slug":"/sensor-systems","permalink":"/my-book/docs/sensor-systems","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"An Overview of the Humanoid Robotics Landscape","permalink":"/my-book/docs/humanoid-robotics-landscape"},"next":{"title":"ROS 2 Services and Actions","permalink":"/my-book/docs/ros2-services-actions"}}');var i=n(2540),r=n(3023);const t={sidebar_position:6},a="A Guide to Humanoid Sensor Systems",c={},l=[{value:"1. Vision: Cameras",id:"1-vision-cameras",level:2},{value:"2. 3D Perception: LiDAR",id:"2-3d-perception-lidar",level:2},{value:"3. Balance and Orientation: IMUs",id:"3-balance-and-orientation-imus",level:2},{value:"4. Touch and Force: Force/Torque Sensors",id:"4-touch-and-force-forcetorque-sensors",level:2}];function d(e){const s={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"a-guide-to-humanoid-sensor-systems",children:"A Guide to Humanoid Sensor Systems"})}),"\n",(0,i.jsx)(s.p,{children:"For a humanoid robot to perceive and navigate its environment, it must rely on a sophisticated suite of sensors. These sensors are the robot's equivalent of human senses, providing the raw data needed to see, hear, feel, and balance. In ROS 2, each of these sensors is typically managed by a dedicated node that publishes its data to the rest of the system."}),"\n",(0,i.jsx)(s.p,{children:"Let's explore the most critical sensor systems for a humanoid robot."}),"\n",(0,i.jsx)(s.h2,{id:"1-vision-cameras",children:"1. Vision: Cameras"}),"\n",(0,i.jsx)(s.p,{children:"Cameras are the primary vision sensor, providing rich, dense information about the environment."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Types"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Monocular Cameras"}),": A single camera, providing a 2D image. Used for object recognition, color detection, and optical flow."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Stereo Cameras"}),": Two cameras spaced a known distance apart. By comparing the two images, the robot can calculate depth and perceive the world in 3D, which is crucial for navigation and grasping."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"ROS Message Type"}),": ",(0,i.jsx)(s.code,{children:"sensor_msgs/msg/Image"})," for raw image data, ",(0,i.jsx)(s.code,{children:"sensor_msgs/msg/CameraInfo"})," for calibration data."]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"2-3d-perception-lidar",children:"2. 3D Perception: LiDAR"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"LiDAR"})," (Light Detection and Ranging) is a sensor that uses laser beams to create a precise, 3D map of the environment."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"How it Works"}),": A LiDAR sensor emits pulses of laser light and measures the time it takes for the light to reflect off objects and return. This allows it to calculate distance with very high accuracy."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Strengths"}),": Excellent for obstacle avoidance, localization (figuring out where the robot is), and mapping. It is unaffected by lighting conditions, unlike cameras."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"ROS Message Type"}),": ",(0,i.jsx)(s.code,{children:"sensor_msgs/msg/LaserScan"})," for 2D LiDAR or ",(0,i.jsx)(s.code,{children:"sensor_msgs/msg/PointCloud2"})," for 3D LiDAR."]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"3-balance-and-orientation-imus",children:"3. Balance and Orientation: IMUs"}),"\n",(0,i.jsxs)(s.p,{children:["An ",(0,i.jsx)(s.strong,{children:"Inertial Measurement Unit (IMU)"})," is a critical sensor for balance and estimating the robot's orientation. It's the robot's inner ear."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Components"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Accelerometer"}),": Measures linear acceleration (changes in velocity). Can also be used to sense the direction of gravity."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Gyroscope"}),": Measures angular velocity (how fast the robot is rotating)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Function"}),": By integrating the data from the accelerometer and gyroscope, the robot can estimate its ",(0,i.jsx)(s.strong,{children:"pose"})," (position and orientation) in 3D space. This is fundamental for bipedal locomotion and keeping the robot from falling over."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"ROS Message Type"}),": ",(0,i.jsx)(s.code,{children:"sensor_msgs/msg/Imu"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"4-touch-and-force-forcetorque-sensors",children:"4. Touch and Force: Force/Torque Sensors"}),"\n",(0,i.jsxs)(s.p,{children:["To manipulate objects with dexterity, a robot needs a sense of touch. ",(0,i.jsx)(s.strong,{children:"Force/Torque (F/T) sensors"})," provide this capability."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Location"}),": These sensors are typically placed in the robot's wrists, ankles, and fingertips."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Function"}),":","\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"In the hands"}),": F/T sensors allow the robot to perform delicate tasks like grasping a fragile object without crushing it. They provide feedback on how much force is being applied."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"In the feet/ankles"}),": They measure the forces exerted by the ground, which is essential for balance control, adjusting to different surfaces (e.g., hard floor vs. soft carpet), and detecting collisions."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"ROS Message Type"}),": ",(0,i.jsx)(s.code,{children:"geometry_msgs/msg/WrenchStamped"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["These sensors, working in concert, provide the robot with a comprehensive understanding of its body and its immediate surroundings. A central challenge in Physical AI is ",(0,i.jsx)(s.strong,{children:"sensor fusion"}),"\u2014the process of combining data from these multiple, imperfect sensor sources to create a single, coherent model of the world."]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},3023(e,s,n){n.d(s,{R:()=>t,x:()=>a});var o=n(3696);const i={},r=o.createContext(i);function t(e){const s=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),o.createElement(r.Provider,{value:s},e.children)}}}]);