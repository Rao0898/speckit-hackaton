"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[7572],{4686(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"speech-and-nlu","title":"Speech Recognition and Natural Language Understanding (NLU)","description":"For a robot to engage in a spoken conversation, it must first be able to convert the sound waves of human speech into a structured understanding of the user\'s intent. This process is a pipeline with two main stages: Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU).","source":"@site/docs/23-speech-and-nlu.md","sourceDirName":".","slug":"/speech-and-nlu","permalink":"/docs/speech-and-nlu","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":23,"frontMatter":{"sidebar_position":23},"sidebar":"tutorialSidebar","previous":{"title":"Integrating GPT Models for Conversational AI in Robots","permalink":"/docs/gpt-for-robotics"},"next":{"title":"The Power of Multi-modal Interaction: Speech, Gesture, and Vision","permalink":"/docs/multimodal-interaction"}}');var s=t(2540),i=t(3023);const r={sidebar_position:23},a="Speech Recognition and Natural Language Understanding (NLU)",c={},l=[{value:"1. Automatic Speech Recognition (ASR)",id:"1-automatic-speech-recognition-asr",level:2},{value:"How ASR Works",id:"how-asr-works",level:3},{value:"ASR in a Robotic Context",id:"asr-in-a-robotic-context",level:3},{value:"2. Natural Language Understanding (NLU)",id:"2-natural-language-understanding-nlu",level:2},{value:"Key Components of NLU",id:"key-components-of-nlu",level:3},{value:"The Rise of LLMs in NLU",id:"the-rise-of-llms-in-nlu",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"speech-recognition-and-natural-language-understanding-nlu",children:"Speech Recognition and Natural Language Understanding (NLU)"})}),"\n",(0,s.jsxs)(n.p,{children:["For a robot to engage in a spoken conversation, it must first be able to convert the sound waves of human speech into a structured understanding of the user's intent. This process is a pipeline with two main stages: ",(0,s.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"})," and ",(0,s.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"1-automatic-speech-recognition-asr",children:"1. Automatic Speech Recognition (ASR)"}),"\n",(0,s.jsx)(n.p,{children:"ASR is the technology that converts spoken audio into text. It is the robotic equivalent of hearing."}),"\n",(0,s.jsx)(n.h3,{id:"how-asr-works",children:"How ASR Works"}),"\n",(0,s.jsx)(n.p,{children:"Modern ASR systems are based on deep learning. They are trained on massive datasets of audio recordings and their corresponding human-verified transcripts. The model learns to map the complex patterns in the audio signal (phonemes, accents, intonations) to words and sentences."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input"}),": An audio stream, typically from a microphone array on the robot."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": A string of text."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"asr-in-a-robotic-context",children:"ASR in a Robotic Context"}),"\n",(0,s.jsx)(n.p,{children:"For a robot, ASR is more challenging than for a smartphone assistant."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise"}),": The robot's own motors and fans generate noise, which can interfere with the microphones."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distance"}),": The user may be speaking to the robot from across the room, which can result in a faint or reverberant signal."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Microphone Arrays"}),": To combat these issues, robots are often equipped with a microphone array. By using signal processing techniques (like ",(0,s.jsx)(n.strong,{children:"beamforming"}),'), the robot can focus its "hearing" in the direction of the person speaking and filter out background noise.']}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Popular ASR toolkits include NVIDIA Riva, and open-source models like Whisper from OpenAI."}),"\n",(0,s.jsx)(n.h2,{id:"2-natural-language-understanding-nlu",children:"2. Natural Language Understanding (NLU)"}),"\n",(0,s.jsxs)(n.p,{children:["Once the user's speech has been converted to text, the NLU system must figure out what that text ",(0,s.jsx)(n.em,{children:"means"}),". NLU is the process of extracting intent and entities from a piece of text."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input"}),': A string of text (e.g., "robot, please bring me the red cup from the kitchen table").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": A structured representation of the user's intent."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-components-of-nlu",children:"Key Components of NLU"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Intent Classification"}),": The NLU model first tries to determine the user's overall goal. In the example above, the intent is ",(0,s.jsx)(n.code,{children:"TransportObject"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Entity Extraction"})," (also called Slot Filling): The model then identifies the key pieces of information (the entities or slots) associated with that intent."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"object_to_transport"}),': "the red cup"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"source_location"}),': "the kitchen table"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"destination_location"}),': "me" (i.e., the user\'s current location)']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The final output of the NLU system might look something like this in a JSON format:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\r\n  "intent": "TransportObject",\r\n  "entities": {\r\n    "object_to_transport": {\r\n      "name": "cup",\r\n      "color": "red"\r\n    },\r\n    "source_location": {\r\n      "name": "table",\r\n      "room": "kitchen"\r\n    },\r\n    "destination_location": "user"\r\n  }\r\n}\n'})}),"\n",(0,s.jsxs)(n.p,{children:["This structured data can then be passed to the robot's task planner, which knows how to execute a ",(0,s.jsx)(n.code,{children:"TransportObject"})," routine."]}),"\n",(0,s.jsx)(n.h2,{id:"the-rise-of-llms-in-nlu",children:"The Rise of LLMs in NLU"}),"\n",(0,s.jsx)(n.p,{children:"Traditionally, NLU systems required training a custom machine learning model for a specific set of intents and entities. You had to provide many examples for each intent you wanted the robot to understand."}),"\n",(0,s.jsx)(n.p,{children:'Large Language Models (LLMs) have dramatically changed this landscape. With a carefully crafted prompt, you can now use a powerful LLM (like GPT-4) to perform NLU in a "zero-shot" or "few-shot" manner, without any specific training.'}),"\n",(0,s.jsx)(n.p,{children:"You can provide the LLM with a prompt that describes the desired intents and entities and give it the user's utterance. The LLM is often capable of correctly parsing the text into the desired structured format on its first try. This makes developing conversational interfaces much faster and more flexible, as you can easily add new capabilities just by updating the prompt."}),"\n",(0,s.jsx)(n.p,{children:"Whether using a traditional NLU model or a modern LLM, the goal is the same: to transform the beautiful, messy, and ambiguous nature of human language into the precise, structured commands that a robot needs to take action."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},3023(e,n,t){t.d(n,{R:()=>r,x:()=>a});var o=t(3696);const s={},i=o.createContext(s);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);