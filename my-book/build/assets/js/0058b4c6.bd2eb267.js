"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[849],{6164(e){e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/docs/gazebo-intro","label":"Introduction to Gazebo","docId":"gazebo-intro","unlisted":false},{"type":"link","href":"/docs/intro","label":"Introduction to ROS 2","docId":"intro","unlisted":false},{"type":"link","href":"/docs/isaac-intro","label":"NVIDIA Isaac","docId":"isaac-intro","unlisted":false},{"type":"link","href":"/docs/vla-intro","label":"Voice-to-Action (VLA) for Humanoids","docId":"vla-intro","unlisted":false},{"type":"link","href":"/docs/nodes-topics","label":"ROS 2 Nodes and Topics","docId":"nodes-topics","unlisted":false},{"type":"link","href":"/docs/lab1-gazebo-urdf","label":"Lab: Simulating a URDF Model in Gazebo","docId":"lab1-gazebo-urdf","unlisted":false},{"type":"link","href":"/docs/physical-ai-foundations","label":"Foundations of Physical AI and Embodied Intelligence","docId":"physical-ai-foundations","unlisted":false},{"type":"link","href":"/docs/digital-to-physical-ai","label":"From Digital AI to Robots that Understand Physical Laws","docId":"digital-to-physical-ai","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics-landscape","label":"An Overview of the Humanoid Robotics Landscape","docId":"humanoid-robotics-landscape","unlisted":false},{"type":"link","href":"/docs/sensor-systems","label":"A Guide to Humanoid Sensor Systems","docId":"sensor-systems","unlisted":false},{"type":"link","href":"/docs/ros2-services-actions","label":"ROS 2 Services and Actions","docId":"ros2-services-actions","unlisted":false},{"type":"link","href":"/docs/ros2-packages-python","label":"Building ROS 2 Packages with Python","docId":"ros2-packages-python","unlisted":false},{"type":"link","href":"/docs/ros2-launch-files-parameters","label":"ROS 2 Launch Files and Parameter Management","docId":"ros2-launch-files-parameters","unlisted":false},{"type":"link","href":"/docs/gazebo-setup","label":"Setting Up the Gazebo Simulation Environment","docId":"gazebo-setup","unlisted":false},{"type":"link","href":"/docs/urdf-sdf","label":"URDF and SDF: Describing Your Robot","docId":"urdf-sdf","unlisted":false},{"type":"link","href":"/docs/gazebo-physics-sensors","label":"Physics and Sensor Simulation in Gazebo","docId":"gazebo-physics-sensors","unlisted":false},{"type":"link","href":"/docs/unity-for-robotics","label":"An Introduction to Unity for Robot Visualization and Simulation","docId":"unity-for-robotics","unlisted":false},{"type":"link","href":"/docs/nvidia-isaac-intro","label":"An Introduction to the NVIDIA Isaac Platform","docId":"nvidia-isaac-intro","unlisted":false},{"type":"link","href":"/docs/isaac-ai-perception-manipulation","label":"AI-Powered Perception and Manipulation with Isaac","docId":"isaac-ai-perception-manipulation","unlisted":false},{"type":"link","href":"/docs/isaac-reinforcement-learning","label":"Reinforcement Learning for Robot Control with Isaac","docId":"isaac-reinforcement-learning","unlisted":false},{"type":"link","href":"/docs/isaac-sim-to-real","label":"Bridging the Gap: Sim-to-Real Transfer Techniques","docId":"isaac-sim-to-real","unlisted":false},{"type":"link","href":"/docs/humanoid-kinematics-dynamics","label":"An Introduction to Humanoid Robot Kinematics and Dynamics","docId":"humanoid-kinematics-dynamics","unlisted":false},{"type":"link","href":"/docs/bipedal-locomotion","label":"The Challenge of Bipedal Locomotion and Balance Control","docId":"bipedal-locomotion","unlisted":false},{"type":"link","href":"/docs/humanoid-manipulation","label":"Humanoid Manipulation and Grasping with Multi-Fingered Hands","docId":"humanoid-manipulation","unlisted":false},{"type":"link","href":"/docs/human-robot-interaction","label":"Designing for Natural Human-Robot Interaction (HRI)","docId":"human-robot-interaction","unlisted":false},{"type":"link","href":"/docs/gpt-for-robotics","label":"Integrating GPT Models for Conversational AI in Robots","docId":"gpt-for-robotics","unlisted":false},{"type":"link","href":"/docs/speech-and-nlu","label":"Speech Recognition and Natural Language Understanding (NLU)","docId":"speech-and-nlu","unlisted":false},{"type":"link","href":"/docs/multimodal-interaction","label":"The Power of Multi-modal Interaction: Speech, Gesture, and Vision","docId":"multimodal-interaction","unlisted":false}]},"docs":{"bipedal-locomotion":{"id":"bipedal-locomotion","title":"The Challenge of Bipedal Locomotion and Balance Control","description":"Enabling a humanoid robot to walk, run, and maintain its balance on two legs is one of the most significant challenges in robotics. Bipedal locomotion is an inherently unstable process, requiring constant, subtle adjustments to prevent the robot from falling over. This section delves into the core concepts of how humanoids achieve stable walking.","sidebar":"tutorialSidebar"},"digital-to-physical-ai":{"id":"digital-to-physical-ai","title":"From Digital AI to Robots that Understand Physical Laws","description":"The transition from digital AI to Physical AI is one of the most significant and challenging frontiers in modern technology. While a digital AI can master a game like chess or Go, which has defined rules and a finite state space, a Physical AI must contend with the infinitely complex and often unforgiving laws of physics.","sidebar":"tutorialSidebar"},"gazebo-intro":{"id":"gazebo-intro","title":"Introduction to Gazebo","description":"Gazebo is a powerful 3D robot simulator that is widely used in robotics research and development. It allows you to accurately and efficiently simulate populations of robots in complex indoor and outdoor environments. Gazebo offers the ability to simulate realistic sensor feedback and apply physics to robot models.","sidebar":"tutorialSidebar"},"gazebo-physics-sensors":{"id":"gazebo-physics-sensors","title":"Physics and Sensor Simulation in Gazebo","description":"One of the greatest strengths of Gazebo is its ability to simulate the laws of physics and the behavior of common robotic sensors. This allows you to test your robot\'s control and perception algorithms in a realistic virtual environment before running them on hardware.","sidebar":"tutorialSidebar"},"gazebo-setup":{"id":"gazebo-setup","title":"Setting Up the Gazebo Simulation Environment","description":"Gazebo is a powerful and widely-used 3D robotics simulator. It allows you to model complex robots and environments, and it includes a high-fidelity physics engine that simulates gravity, friction, and collisions. For ROS 2 developers, Gazebo is an essential tool for testing and developing robot software before deploying it on physical hardware.","sidebar":"tutorialSidebar"},"gpt-for-robotics":{"id":"gpt-for-robotics","title":"Integrating GPT Models for Conversational AI in Robots","description":"The advent of powerful Large Language Models (LLMs) like OpenAI\'s GPT (Generative Pre-trained Transformer) series has opened up revolutionary new possibilities for human-robot interaction. By integrating these models, we can move beyond simple, pre-programmed commands and create robots that can understand and respond to natural, high-level human language.","sidebar":"tutorialSidebar"},"human-robot-interaction":{"id":"human-robot-interaction","title":"Designing for Natural Human-Robot Interaction (HRI)","description":"For humanoid robots to become true assistants and collaborators in our daily lives, they must be more than just capable; they must be approachable, predictable, and intuitive to interact with. Human-Robot Interaction (HRI) is the field of study dedicated to understanding, designing, and evaluating robotic systems for use by or with humans.","sidebar":"tutorialSidebar"},"humanoid-kinematics-dynamics":{"id":"humanoid-kinematics-dynamics","title":"An Introduction to Humanoid Robot Kinematics and Dynamics","description":"Building a humanoid robot that can move gracefully and perform useful tasks requires a deep understanding of its kinematics and dynamics. These two fields of classical mechanics provide the mathematical foundation for modeling and controlling the robot\'s motion.","sidebar":"tutorialSidebar"},"humanoid-manipulation":{"id":"humanoid-manipulation","title":"Humanoid Manipulation and Grasping with Multi-Fingered Hands","description":"While bipedal locomotion enables a humanoid robot to navigate our world, its ability to perform useful tasks ultimately depends on its capacity for manipulation\u2014the act of purposefully interacting with objects using its hands. Humanoid manipulation is an incredibly complex field, as it aims to replicate the astonishing dexterity of the human hand.","sidebar":"tutorialSidebar"},"humanoid-robotics-landscape":{"id":"humanoid-robotics-landscape","title":"An Overview of the Humanoid Robotics Landscape","description":"Humanoid robotics is one of the most ambitious and captivating fields within Physical AI. The goal is to create robots that mimic the form and, ultimately, the capabilities of the human body. This form factor is not chosen for novelty; it is chosen because the world we have built\u2014our homes, our tools, our cities\u2014is designed for humans. A robot that can navigate and interact in a human-centric environment has the potential to be a truly general-purpose assistant.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction to ROS 2","description":"Welcome to the exciting world of the Robot Operating System 2 (ROS 2)! ROS 2 is a flexible framework for writing robot software. It\'s not an operating system in the traditional sense, but rather a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behaviors across a wide variety of robotic platforms.","sidebar":"tutorialSidebar"},"isaac-ai-perception-manipulation":{"id":"isaac-ai-perception-manipulation","title":"AI-Powered Perception and Manipulation with Isaac","description":"The NVIDIA Isaac platform is designed to tackle two of the most challenging domains in robotics: perception (understanding the world through sensors) and manipulation (physically interacting with the world). Isaac provides a suite of powerful, GPU-accelerated tools and pre-trained models, known as GEMs (Isaac ROS hardware-accelerated packages), to solve these problems.","sidebar":"tutorialSidebar"},"isaac-intro":{"id":"isaac-intro","title":"NVIDIA Isaac","description":"NVIDIA Isaac is a powerful platform for accelerating the development and deployment of AI-powered robots. It provides a comprehensive set of tools, SDKs, and a simulation environment designed to streamline the entire robotics workflow, from perception and navigation to manipulation and human-robot interaction.","sidebar":"tutorialSidebar"},"isaac-reinforcement-learning":{"id":"isaac-reinforcement-learning","title":"Reinforcement Learning for Robot Control with Isaac","description":"Reinforcement Learning (RL) is a powerful paradigm in machine learning where an \\"agent\\" learns to make decisions by performing actions in an environment to maximize a cumulative \\"reward.\\" For robotics, RL is particularly exciting because it allows a robot to learn complex behaviors through trial and error, without needing to be explicitly programmed.","sidebar":"tutorialSidebar"},"isaac-sim-to-real":{"id":"isaac-sim-to-real","title":"Bridging the Gap: Sim-to-Real Transfer Techniques","description":"One of the ultimate goals of using a simulator like Isaac Sim is to train a robot in a virtual environment and then have it work successfully in the real world. This process is known as sim-to-real transfer. However, there is always a \\"reality gap\\" between simulation and the real world. No simulator is perfect.","sidebar":"tutorialSidebar"},"lab1-gazebo-urdf":{"id":"lab1-gazebo-urdf","title":"Lab: Simulating a URDF Model in Gazebo","description":"This lab will guide you through launching a simple URDF (Unified Robot Description Format) model in Gazebo and interacting with it using ROS 2.","sidebar":"tutorialSidebar"},"multimodal-interaction":{"id":"multimodal-interaction","title":"The Power of Multi-modal Interaction: Speech, Gesture, and Vision","description":"Human communication is rarely limited to a single channel. When we talk, we also use our hands to gesture, our eyes to make contact, and our posture to convey meaning. For a robot to be a truly natural and effective collaborator, it must also be able to understand and use multi-modal interaction\u2014the seamless blending of multiple communication channels, primarily speech, gesture, and vision.","sidebar":"tutorialSidebar"},"nodes-topics":{"id":"nodes-topics","title":"ROS 2 Nodes and Topics","description":"In ROS 2, the fundamental unit of computation is a Node. Nodes are essentially executable processes that perform a specific task within the robot system. For example, you might have one node for reading data from a camera, another for controlling a motor, and a third for navigating.","sidebar":"tutorialSidebar"},"nvidia-isaac-intro":{"id":"nvidia-isaac-intro","title":"An Introduction to the NVIDIA Isaac Platform","description":"The NVIDIA Isaac platform is a powerful, end-to-end toolkit for the development, simulation, and deployment of AI-powered robots. It provides a suite of hardware and software tools designed to accelerate the creation of robust and intelligent robotic systems. At its core, the Isaac platform is built to leverage NVIDIA\'s deep expertise in GPU-accelerated computing and AI.","sidebar":"tutorialSidebar"},"physical-ai-foundations":{"id":"physical-ai-foundations","title":"Foundations of Physical AI and Embodied Intelligence","description":"Welcome to the foundational principles of Physical AI. While digital AI, the kind you might be familiar with through chatbots or image generators, exists purely in the realm of data and algorithms, Physical AI takes a giant leap into the material world. It is the science and engineering of creating intelligent agents that can perceive, reason about, and interact with their physical environment in a purposeful way.","sidebar":"tutorialSidebar"},"ros2-launch-files-parameters":{"id":"ros2-launch-files-parameters","title":"ROS 2 Launch Files and Parameter Management","description":"As your robotic systems grow in complexity, running each node in a separate terminal becomes tedious and unmanageable. ROS 2 provides a powerful launch system that allows you to start and configure multiple nodes at once from a single command. Additionally, the launch system provides a robust way to manage parameters, which are configurable settings for your nodes.","sidebar":"tutorialSidebar"},"ros2-packages-python":{"id":"ros2-packages-python","title":"Building ROS 2 Packages with Python","description":"In ROS 2, code is organized into packages. A package is a directory containing your source code (e.g., Python scripts), launch files, configuration files, and a package.xml file that provides metadata about the package. The build system used by ROS 2 is called colcon.","sidebar":"tutorialSidebar"},"ros2-services-actions":{"id":"ros2-services-actions","title":"ROS 2 Services and Actions","description":"While ROS 2 topics are perfect for continuous data streams, there are many situations where a request-response or long-running task paradigm is more appropriate. For these scenarios, ROS 2 provides two powerful communication patterns: Services and Actions.","sidebar":"tutorialSidebar"},"sensor-systems":{"id":"sensor-systems","title":"A Guide to Humanoid Sensor Systems","description":"For a humanoid robot to perceive and navigate its environment, it must rely on a sophisticated suite of sensors. These sensors are the robot\'s equivalent of human senses, providing the raw data needed to see, hear, feel, and balance. In ROS 2, each of these sensors is typically managed by a dedicated node that publishes its data to the rest of the system.","sidebar":"tutorialSidebar"},"speech-and-nlu":{"id":"speech-and-nlu","title":"Speech Recognition and Natural Language Understanding (NLU)","description":"For a robot to engage in a spoken conversation, it must first be able to convert the sound waves of human speech into a structured understanding of the user\'s intent. This process is a pipeline with two main stages: Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU).","sidebar":"tutorialSidebar"},"unity-for-robotics":{"id":"unity-for-robotics","title":"An Introduction to Unity for Robot Visualization and Simulation","description":"While Gazebo is the workhorse for physics-based simulation in the ROS ecosystem, the Unity game engine has emerged as a powerful alternative, particularly for creating high-fidelity graphics, rich human-robot interaction scenarios, and large-scale, visually realistic environments.","sidebar":"tutorialSidebar"},"urdf-sdf":{"id":"urdf-sdf","title":"URDF and SDF: Describing Your Robot","description":"To simulate a robot in Gazebo or visualize it in RViz, you first need a way to describe its physical structure. ROS 2 and Gazebo use two primary formats for this: the Unified Robot Description Format (URDF) and the Simulation Description Format (SDF).","sidebar":"tutorialSidebar"},"vla-intro":{"id":"vla-intro","title":"Voice-to-Action (VLA) for Humanoids","description":"Integrating voice commands into humanoid robots transforms how humans interact with intelligent machines. Voice-to-Action (VLA) systems enable robots to understand spoken language, interpret intent, and execute physical tasks, bridging the gap between human instruction and robotic behavior.","sidebar":"tutorialSidebar"}}}}')}}]);